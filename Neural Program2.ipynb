{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxdt0_Qj5vTI",
        "outputId": "14221bb3-7e8a-4930-acb0-14cf5146bcb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 0s 0us/step\n",
            "x_train shape: (25000,)\n",
            "y_train shape: (25000,)\n",
            "x_test shape: (25000,)\n",
            "y_test shape: (25000,)\n"
          ]
        }
      ],
      "source": [
        "from keras.datasets import imdb\n",
        "\n",
        "# Load the IMDb movie reviews dataset\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data()\n",
        "\n",
        "# Print the shape of the datasets\n",
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"x_test shape:\", x_test.shape)\n",
        "print(\"y_test shape:\", y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Set the maximum number of words\n",
        "max_words = 10000\n",
        "\n",
        "# Load the IMDb movie reviews dataset\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_words)\n",
        "\n",
        "# Pad sequences to a maximum length\n",
        "max_length = 200\n",
        "x_train = pad_sequences(x_train, maxlen=max_length)\n",
        "x_test = pad_sequences(x_test, maxlen=max_length)\n",
        "\n",
        "# Print the shape of the preprocessed datasets\n",
        "print(\"x_train shape after padding:\", x_train.shape)\n",
        "print(\"x_test shape after padding:\", x_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2CmJuuF5-ND",
        "outputId": "7be8d176-79f4-4d99-8e86-349180b29b89"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape after padding: (25000, 200)\n",
            "x_test shape after padding: (25000, 200)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "# Define the model\n",
        "model = Sequential()\n",
        "\n",
        "# Add an embedding layer\n",
        "embedding_size = 128\n",
        "model.add(Embedding(input_dim=max_words, output_dim=embedding_size, input_length=max_length))\n",
        "\n",
        "# Add an LSTM layer\n",
        "lstm_units = 64\n",
        "model.add(LSTM(units=lstm_units, dropout=0.2, recurrent_dropout=0.2))\n",
        "\n",
        "# Add a dense layer\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6AlEIj2e6anT",
        "outputId": "3c9abb35-eae1-40dc-830c-37344c5df079"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 200, 128)          1280000   \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 64)                49408     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1329473 (5.07 MB)\n",
            "Trainable params: 1329473 (5.07 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the RNN model\n",
        "batch_size = 128\n",
        "epochs = 5\n",
        "\n",
        "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIBfbcsD6pmx",
        "outputId": "e5287282-5c98-46aa-b5ed-2424987c1945"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "196/196 [==============================] - 168s 838ms/step - loss: 0.4302 - accuracy: 0.7978 - val_loss: 0.3323 - val_accuracy: 0.8622\n",
            "Epoch 2/5\n",
            "196/196 [==============================] - 167s 853ms/step - loss: 0.2347 - accuracy: 0.9066 - val_loss: 0.3042 - val_accuracy: 0.8682\n",
            "Epoch 3/5\n",
            "196/196 [==============================] - 165s 843ms/step - loss: 0.1706 - accuracy: 0.9374 - val_loss: 0.3381 - val_accuracy: 0.8596\n",
            "Epoch 4/5\n",
            "196/196 [==============================] - 157s 799ms/step - loss: 0.1465 - accuracy: 0.9455 - val_loss: 0.3738 - val_accuracy: 0.8642\n",
            "Epoch 5/5\n",
            "196/196 [==============================] - 156s 799ms/step - loss: 0.1130 - accuracy: 0.9600 - val_loss: 0.4373 - val_accuracy: 0.8553\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "# Report accuracy\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "grh3iIRk7XV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "# Define the function to create the RNN model\n",
        "def create_rnn_model(learning_rate=0.001, optimizer='adam'):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=max_words, output_dim=embedding_size, input_length=max_length))\n",
        "    model.add(LSTM(units=lstm_units, dropout=0.2, recurrent_dropout=0.2))\n",
        "    model.add(Dense(units=1, activation='sigmoid'))\n",
        "    model.compile(optimizer=optimizer(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Wrap the Keras model with a scikit-learn compatible wrapper\n",
        "model = KerasClassifier(build_fn=create_rnn_model, verbose=0)\n",
        "\n",
        "# Define the grid search parameters\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.001],\n",
        "    'batch_size': [64, 128],\n",
        "    'optimizer': [Adam]\n",
        "}\n",
        "\n",
        "# Perform grid search\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3)\n",
        "grid_result = grid.fit(x_train, y_train)\n",
        "\n",
        "# Summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n"
      ],
      "metadata": {
        "id": "jf1TaMEC7qEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XBTlB-Hs8Kl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ReduceLROnPlateau\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=max_words, output_dim=embedding_size, input_length=max_length))\n",
        "model.add(LSTM(units=lstm_units, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define callbacks\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.0001)\n",
        "\n",
        "# Train the model with callbacks\n",
        "history = model.fit(x_train, y_train, batch_size=128, epochs=5, validation_data=(x_test, y_test), callbacks=[reduce_lr])\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', loss)\n",
        "print('Test accuracy:', accuracy)\n",
        "\n",
        "# Result visualization\n",
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "oxpeYyaE8dPo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}